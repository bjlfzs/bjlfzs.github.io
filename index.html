<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Zequn Yang</title>
  
  <meta name="author" content="Jon Barron">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="images/favicon.ico">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Zequn Yang (杨泽群)</name>
              </p>
              <p>I am a second year PhD student at <a href="http://ai.ruc.edu.cn/">Gaoling School of Artificial Intelligence</a>, Renmin University of China. I am advised by <a href="https://dtaoo.github.io/">Prof. Di Hu</a>, and co-adviced by Prof. Feiping Nie. Now my research interests focus on the mechanism of multi-view and multi-modal learning.
              </p>

              <p>I received my bachelor's degree in Automation from <a href="https://www.buaa.edu.cn/"> Beihang University </a> in 2022.
              </p>

              <p style="text-align:center">
                <a href="mailto:zqyang@ruc.edu.cn">Email</a> &nbsp/&nbsp
                <!--
                <a href="data/JonBarron-CV.pdf">CV</a> &nbsp/&nbsp
                <a href="data/JonBarron-bio.txt">Bio</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?hl=en&user=jktWnL8AAAAJ">Google Scholar</a> &nbsp/&nbsp
                <a href="https://twitter.com/jon_barron">Twitter</a> &nbsp/&nbsp  -->
                <a href="https://github.com/bjlfzs/">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:30%;max-width:30%">
              <a href="images/theq.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/theq.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>News</heading>
            <p>
              <strong>[2022-08]</strong> We wrote an article about recent advances in audio-visual learning! <a href="https://gewu-lab.github.io/audio-visual-learning/">[website]</a>
            </p>
            <p>
              <strong>[2022-05]</strong> Gave a talk @ <a href="https://2022.baai.ac.cn/">2022 BAAI Conference</a> . Please find slides <a href="https://zenodo.org/record/6597495/files/2022%E6%99%BA%E6%BA%90%E5%A4%A7%E4%BC%9A-%E5%8D%AB%E9%9B%85%E7%8F%82.pdf?download=1">here</a>!
            </p>
              <strong>[2022-03]</strong> Two papers accepted by CVPR 2022, thanks to all co-authors!
            </p>
            <p>
              <strong>[2021-12]</strong> One paper accepted by TPAMI, thanks to all co-authors!
            </p>
            <p>
              <strong>[2021-06]</strong> Graduate from University of Electronic Science and Technology of China!
            </p>
          </td>
        </tr>
      </tbody></table>

      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tbody>
        <tr>
        <td style="padding:20px;width:100%;vertical-align:middle">
          <heading>Services</heading>
          <p>
            <strong>Conference Reviewer:</strong> CVPR 2022-2023, ECCV 2022, AAAI 2023
          </p>
          <p>
            <strong>Journal Reviewer:</strong> TMM
          </p>
        </td>
      </tr>
    </tbody></table>

    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <heading style="padding:20px">Preprint</heading>
      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src="images/survey.jpg" alt="clean-usnob" width="320" height="160">
        </td>
        <td width="75%" valign="middle">
          <papertitle>Learning in Audio-visual Context: A Review, Analysis, and New Perspective</papertitle>
          <br>
          <br>
          <strong>Yake Wei</strong>, <a href="https://dtaoo.github.io">Di Hu</a>, <a href="https://yapengtian.org/">Yapeng Tian</a>, <a href="https://scholar.google.com/citations?user=ahUibskAAAAJ&hl=zh-CN">Xuelong Li</a>
          <br>
          <br>
          <em>Under review</em>
          <br>
          <a href="https://arxiv.org/abs/2208.09579">arXiv</a> / <a href="https://gewu-lab.github.io/audio-visual-learning/">website</a> / <a href="https://gewu-lab.github.io/awesome-audiovisual-learning/">awesome list</a> 
          <br>
          <p>A systematical survey about the audio-visual learning field.</p>
        </td>
      </tr>


    </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <heading style="padding:20px">Publications</heading>(&#42; equal contribution)

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/balance2022.jpg" alt="clean-usnob" width="320" height="160">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Balanced Multimodal Learning via On-the-fly Gradient Modulation</papertitle>
              <br>
              <br>
              Xiaokang Peng*, <strong>Yake Wei*</strong>, <a href="https://antony0621.github.io/">Andong Deng</a>, Dong Wang, <a href="https://dtaoo.github.io">Di Hu</a>
              <br>
              <br>
              <em>CVPR</em>, 2022 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
              <br>
              <a href="https://arxiv.org/pdf/2203.15332.pdf">arXiv</a> / <a href="https://github.com/GeWu-Lab/OGM-GE_CVPR2022">code
              </a>
              <br>
              <p>Alleviate optimization imbalance in multi-modal learning via on-the-fly gradient modulation.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/avqa2022.jpg" alt="clean-usnob" width="320" height="160">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Learning to Answer Questions in Dynamic Audio-Visual Scenarios</papertitle>
              <br>
              <br>
              <a href="https://ayameyao.github.io/">Guangyao Li*</a>, <strong>Yake Wei*</strong>, <a href="https://yapengtian.org/">Yapeng Tian*</a>, <a href="https://www.cs.rochester.edu/~cxu22/">Chenliang Xu</a>, <a href="https://scholar.google.com/citations?user=tbxCHJgAAAAJ">Ji-Rong Wen</a>, <a href="https://dtaoo.github.io">Di Hu</a>
              <br>
              <br>
              <em>CVPR</em>, 2022 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
              <br>
              <a href="https://arxiv.org/pdf/2203.14072.pdf">arXiv</a> / <a href="https://gewu-lab.github.io/MUSIC-AVQA/">project page
              </a>
              <br>
              <p>Audio-Visual Question Answering and propose MUSIC-AVQA dataset.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/pami2021.jpg" alt="clean-usnob" width="320" height="160">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Class-aware Sounding Objects Localization via Audiovisual Correspondence</papertitle>
              <br>
              <br>
              <a href="https://dtaoo.github.io">Di Hu</a>, <strong>Yake Wei</strong>, <a href="https://shvdiwnkozbw.github.io/">Rui Qian</a>, <a href="https://weiyaolin.github.io/">Weiyao Lin</a>, <a href="https://scholar.google.com/citations?user=v5LctN8AAAAJ&hl">Ruihua Song</a>, <a href="https://scholar.google.com/citations?user=tbxCHJgAAAAJ">Ji-Rong Wen</a>
              <br>
              <br>
              <em>TPAMI</em>
              <br>
              <a href="https://arxiv.org/pdf/2112.11749.pdf">arXiv</a> / <a href="https://gewu-lab.github.io/CSOL_TPAMI2021/">project page
              </a>
              <br>
              <p>Discriminative sounding objects localization.</p>
            </td>
          </tr>

        </tbody></table>



        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <p font-size:small;>
                <br>
                <br>
                <div style="float:left;">
                    Updated at Jan. 2023
                </div>
                <div style="float:right;">
                    Thanks <a href="https://jonbarron.info">Jon Barron</a> for this amazing template.
                </div>
                <br>
                <br>        
            </p>                           
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
